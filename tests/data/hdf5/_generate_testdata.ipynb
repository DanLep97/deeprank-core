{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pkg_resources as pkg\n",
    "PATH_DEEPRANK_CORE = Path(pkg.resource_filename(\"deeprankcore\", \"\"))\n",
    "ROOT = PATH_DEEPRANK_CORE.parent\n",
    "PATH_TEST = ROOT / \"tests\"\n",
    "from deeprankcore.query import (\n",
    "    QueryCollection,\n",
    "    ProteinProteinInterfaceResidueQuery,\n",
    "    SingleResidueVariantResidueQuery,\n",
    "    ProteinProteinInterfaceAtomicQuery)\n",
    "from deeprankcore.tools.target import compute_targets\n",
    "from deeprankcore.dataset import save_hdf5_keys\n",
    "from deeprankcore.domain.aminoacidlist import alanine, phenylalanine\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating 1ATN_ppi.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", BiopythonWarning)\n",
    "    warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "    ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "    pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "    pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "    chain_id1 = \"A\"\n",
    "    chain_id2 = \"B\"\n",
    "    pdb_paths = [\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\")]\n",
    "\n",
    "    queries = QueryCollection()\n",
    "\n",
    "    for pdb_path in pdb_paths:\n",
    "        # Append data points\n",
    "        targets = compute_targets(pdb_path, ref_path)\n",
    "        queries.add(ProteinProteinInterfaceResidueQuery(\n",
    "            pdb_path = pdb_path,\n",
    "            chain_id1 = chain_id1,\n",
    "            chain_id2 = chain_id2,\n",
    "            targets = targets,\n",
    "            pssm_paths = {\n",
    "                chain_id1: pssm_path1,\n",
    "                chain_id2: pssm_path2\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # Generate graphs and save them in hdf5 files\n",
    "    output_paths = queries.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating train.hdf5, valid.hdf5, test.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 pdbs found.\n",
      "100 MHC pssms found.\n",
      "100 peptide pssms found.\n",
      "Adding 100 queries to the query collection ...\n",
      "Queries created and ready to be processed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chain M not found in the structure\n",
      "Query residue-ppi-BA-101047:M-P's graph was not saved in the hdf5 file; check the query's files\n",
      "tri_norm: face with normal vector of lenght 0\n",
      "tri_norm: face with normal vector of lenght 0\n",
      "tri_norm: face with normal vector of lenght 0\n",
      "tri_norm: face with normal vector of lenght 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed-queries.hdf5']\n"
     ]
    }
   ],
   "source": [
    "# Local data\n",
    "project_folder = '/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/snellius_data/snellius_100_07122022/'\n",
    "csv_file_name = 'BA_pMHCI_human_quantitative.csv'\n",
    "models_folder_name = 'exp_nmers_all_HLA_quantitative'\n",
    "data = 'pMHCI'\n",
    "resolution = 'residue' # either 'residue' or 'atomic'\n",
    "distance_cutoff = 15 # max distance in Ã… between two interacting residues/atoms of two proteins\n",
    "\n",
    "csv_file_path = f'{project_folder}data/external/processed/I/{csv_file_name}'\n",
    "models_folder_path = f'{project_folder}data/{data}/features_input_folder/{models_folder_name}'\n",
    "\n",
    "pdb_files = glob.glob(os.path.join(models_folder_path + '/pdb', '*.pdb'))\n",
    "pdb_files.sort()\n",
    "print(f'{len(pdb_files)} pdbs found.')\n",
    "pssm_m = glob.glob(os.path.join(models_folder_path + '/pssm', '*.M.*.pssm'))\n",
    "pssm_m.sort()\n",
    "print(f'{len(pdb_files)} MHC pssms found.')\n",
    "pssm_p = glob.glob(os.path.join(models_folder_path + '/pssm', '*.P.*.pssm'))\n",
    "pssm_p.sort()\n",
    "print(f'{len(pdb_files)} peptide pssms found.')\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "csv_data.cluster = csv_data.cluster.fillna(-1)\n",
    "pdb_ids_csv = [pdb_file.split('/')[-1].split('.')[0].replace('-', '_') for pdb_file in pdb_files]\n",
    "clusters = [csv_data[csv_data.ID == pdb_id].cluster.values[0] for pdb_id in pdb_ids_csv]\n",
    "bas = [csv_data[csv_data.ID == pdb_id].measurement_value.values[0] for pdb_id in pdb_ids_csv]\n",
    "\n",
    "queries = QueryCollection()\n",
    "print(f'Adding {len(pdb_files)} queries to the query collection ...')\n",
    "for i in range(len(pdb_files)):\n",
    "    queries.add(\n",
    "        ProteinProteinInterfaceResidueQuery(\n",
    "            pdb_path = pdb_files[i], \n",
    "            chain_id1 = \"M\",\n",
    "            chain_id2 = \"P\",\n",
    "            distance_cutoff = distance_cutoff,\n",
    "            targets = {\n",
    "                'binary': int(float(bas[i]) <= 500), # binary target value\n",
    "                'BA': bas[i], # continuous target value\n",
    "                'cluster': clusters[i]\n",
    "                },\n",
    "            pssm_paths = {\n",
    "                \"M\": pssm_m[i],\n",
    "                \"P\": pssm_p[i]\n",
    "                }))\n",
    "print(f'Queries created and ready to be processed.\\n')\n",
    "\n",
    "output_paths = queries.process()\n",
    "print(output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing hdf5 file in train, valid, test\n",
    "hdf5_path = 'residue.hdf5'\n",
    "train_clusters = [3, 4, 5, 2]\n",
    "val_clusters = [1, 8]\n",
    "test_clusters = [6]\n",
    "target = 'target_values'\n",
    "feature = 'cluster'\n",
    "\n",
    "clusters = {}\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "# '/Users/giuliacrocioni/remote_snellius/data/pMHCI/features_output_folder/GNN/residue/13072022/residue.hdf5'\n",
    "with h5py.File(hdf5_path, 'r') as hdf5:\n",
    "\n",
    "    for key in hdf5.keys():\n",
    "        feature_value = float(hdf5[key][target][feature][()])\n",
    "        if feature_value in train_clusters:\n",
    "            train_ids.append(key)\n",
    "        elif feature_value in val_clusters:\n",
    "            val_ids.append(key)\n",
    "        elif feature_value in test_clusters:\n",
    "            test_ids.append(key)\n",
    "\n",
    "        if feature_value in clusters.keys():\n",
    "            clusters[int(feature_value)] += 1\n",
    "        else:\n",
    "            clusters[int(feature_value)] = 1\n",
    "\n",
    "\n",
    "    print(f'Trainset contains {len(train_ids)} data points, {round(100*len(train_ids)/len(hdf5.keys()), 2)}% of the total data.')\n",
    "    print(f'Validation set contains {len(val_ids)} data points, {round(100*len(val_ids)/len(hdf5.keys()), 2)}% of the total data.')\n",
    "    print(f'Test set contains {len(test_ids)} data points, {round(100*len(test_ids)/len(hdf5.keys()), 2)}% of the total data.\\n')\n",
    "\n",
    "    for (key, value) in dict(sorted(clusters.items(), key=lambda x:x[1], reverse=True)).items():\n",
    "        print(f'Group with value {key}: {value} data points, {round(100*value/len(hdf5.keys()), 2)}% of total data.')\n",
    "\n",
    "save_hdf5_keys(hdf5_path, train_ids, 'train.hdf5', hardcopy = True)\n",
    "save_hdf5_keys(hdf5_path, val_ids, 'valid.hdf5', hardcopy = True)\n",
    "save_hdf5_keys(hdf5_path, test_ids, 'test.hdf5', hardcopy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating variants.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_queries = 5\n",
    "pdb_path = str(PATH_TEST / \"data/pdb/3C8P/3C8P.pdb\")\n",
    "ref_path = str(PATH_TEST / \"data/ref/3C8P/3C8P.pdb\")\n",
    "targets = compute_targets(pdb_path, ref_path)\n",
    "queries = QueryCollection()\n",
    "\n",
    "for number in range(1, count_queries + 1):\n",
    "    query = SingleResidueVariantResidueQuery(\n",
    "        pdb_path,\n",
    "        \"A\",\n",
    "        number,\n",
    "        None,\n",
    "        alanine,\n",
    "        phenylalanine,\n",
    "        pssm_paths={\n",
    "            \"A\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.A.pdb.pssm\"),\n",
    "            \"B\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.B.pdb.pssm\")},\n",
    "        targets = targets\n",
    "    )\n",
    "    queries.add(query)\n",
    "\n",
    "output_paths = queries.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating atomic.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "chain_id1 = \"A\"\n",
    "chain_id2 = \"B\"\n",
    "pdb_paths = [\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\")]\n",
    "\n",
    "queries = QueryCollection()\n",
    "\n",
    "for pdb_path in pdb_paths:\n",
    "    # Append data points\n",
    "    targets = compute_targets(pdb_path, ref_path)\n",
    "    queries.add(ProteinProteinInterfaceAtomicQuery(\n",
    "        pdb_path = pdb_path,\n",
    "        chain_id1 = chain_id1,\n",
    "        chain_id2 = chain_id2,\n",
    "        targets = targets,\n",
    "        pssm_paths = {\n",
    "            chain_id1: pssm_path1,\n",
    "            chain_id2: pssm_path2\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Generate graphs and save them in hdf5 files\n",
    "output_paths = queries.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deeprankcore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9044f9e9bb2984c6e66c1f9356a97652bc688d95c3fd9413d1ef7214abf67660"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
