{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pkg_resources as pkg\n",
    "PATH_DEEPRANK_CORE = Path(pkg.resource_filename(\"deeprankcore\", \"\"))\n",
    "ROOT = PATH_DEEPRANK_CORE.parent\n",
    "PATH_TEST = ROOT / \"tests\"\n",
    "from deeprankcore.query import (\n",
    "    QueryCollection,\n",
    "    ProteinProteinInterfaceResidueQuery,\n",
    "    SingleResidueVariantResidueQuery,\n",
    "    ProteinProteinInterfaceAtomicQuery)\n",
    "from deeprankcore.tools.target import compute_targets\n",
    "from deeprankcore.DataSet import save_hdf5_keys\n",
    "from deeprankcore.domain.aminoacidlist import alanine, phenylalanine\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating 1ATN_ppi.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", BiopythonWarning)\n",
    "    warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "    ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "    pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "    pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "    chain_id1 = \"A\"\n",
    "    chain_id2 = \"B\"\n",
    "    pdb_paths = [\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\")]\n",
    "\n",
    "    queries = QueryCollection()\n",
    "\n",
    "    for pdb_path in pdb_paths:\n",
    "        # Append data points\n",
    "        targets = compute_targets(pdb_path, ref_path)\n",
    "        queries.add(ProteinProteinInterfaceResidueQuery(\n",
    "            pdb_path = pdb_path,\n",
    "            chain_id1 = chain_id1,\n",
    "            chain_id2 = chain_id2,\n",
    "            targets = targets,\n",
    "            pssm_paths = {\n",
    "                chain_id1: pssm_path1,\n",
    "                chain_id2: pssm_path2\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # Generate graphs and save them in hdf5 files\n",
    "    output_paths = queries.process(queries, process_count=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating train.hdf5, valid.hdf5, test.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions, we'll rewrite them\n",
    "def getBestScorePdbs(pdb_models_folder):\n",
    "\t'''\n",
    "\tTakes as input the folder containing the .pdb models generated by Pandora, \n",
    "\tand select the best one from the .tsv file.\n",
    "\tThe .tsv file contains the models' scores and is located in the same pdbs' folder.\n",
    "\tReturns a list containing such pdbs' paths.\n",
    "\t'''\n",
    "\tpdbs_list = [fname.split('molpdf_DOPE.tsv')[0]+open(fname).read().split('\\t')[0] \\\n",
    "\t\t\t for fname in glob.glob(f'{pdb_models_folder}/*/*/molpdf_DOPE.tsv')]\n",
    "\treturn pdbs_list\n",
    "\n",
    "def getPssms(pdbs_list):\n",
    "    '''\n",
    "    Takes as input a list containing the selected pdbs' paths.\n",
    "    \n",
    "    Returns two lists, containing pssms' files of MHCs and peptides, respectively. \n",
    "    '''\n",
    "    pssm_m = [glob.glob(pdb.replace('models/', 'pssm_mapped/').replace(pdb.split('/')[-1], '') \\\n",
    "\t\t\t + 'pssm/*M*.pssm')[0] for pdb in pdbs_list]\n",
    "    pssm_p = [glob.glob(pdb.replace('models/', 'pssm_mapped/').replace(pdb.split('/')[-1], '') \\\n",
    "\t\t\t +'pssm/*P*.pssm')[0] for pdb in pdbs_list]\n",
    "    return pssm_m, pssm_p\n",
    "\n",
    "def processCsv(csv_file_path):\n",
    "\t'''\n",
    "\tTakes as input the .csv file contaning {task} data.\n",
    "\tReturns lists of ids, alleles, peptides, scores, cluster.\n",
    "\t'''\n",
    "\tcsv = open(csv_file_path).read().split('\\n')[1:-1]\n",
    "\tcsv = [i.split(',')for i in csv]\n",
    "\tcsv_transposed = [[row[column] for row in csv]for column in range(len(csv[0]))]\n",
    "\treturn csv_transposed[8], csv_transposed[0], csv_transposed[1], csv_transposed[2], csv_transposed[10]\n",
    "\n",
    "def getPilotTargets(pdbs_list, csv_file_path):\n",
    "    '''\n",
    "\tTakes as input the pdbs list and the .csv containing {task} data.\n",
    "\tReturns lists of {task} targets for each pdb in pdbs_list, and the corresponding clusters. \n",
    "\t'''\n",
    "\n",
    "    pattern = r'(BA[_]\\w+)[.]\\w+[.]pdb'\n",
    "    # Finds BA_xxxxx id in each pdb path's, and uses it to retrieve the corresponding target\n",
    "    # Then pdb_targets variable will contain the targets according to pdbs_list paths' order\n",
    "    pdb_ids = re.compile(pattern)\n",
    "    \n",
    "    ids, _, _, scores, clusters = processCsv(csv_file_path)\n",
    "    # Creates binary targets, keeping also the continuous value of {task} as second element of each sublist\n",
    "    targets = [[int(float(score) <= 500), float(score)] for _, score in enumerate(scores)]\n",
    "    pdb_targets = [targets[ids.index(pdb_ids.findall(pdb)[0])] \\\n",
    "        for pdb in pdbs_list]\n",
    "    return pdb_targets, clusters\n",
    "\n",
    "# Local data\n",
    "project_folder = '/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/snellius_50/'\n",
    "distance_cutoff = 15 # max distance in Ã… between two interacting residues/atoms of two proteins\n",
    "\n",
    "pdb_models_folder = f'{project_folder}data/pMHCI/models/BA/'\n",
    "csv_file_path = f'{project_folder}data/binding_data/BA_pMHCI.csv'\n",
    "\n",
    "print('Script running has started ...')\n",
    "pdbs_list = getBestScorePdbs(pdb_models_folder)\n",
    "print(f'pdbs files paths loaded, {len(pdbs_list)} pdbs found.')\n",
    "pssm_m, pssm_p = getPssms(pdbs_list)\n",
    "print(f'pssms files paths loaded, {len(pssm_m)} and {len(pssm_p)} files found for M and P, respectively.\\n')\n",
    "pdb_targets, clusters = getPilotTargets(pdbs_list, csv_file_path)\n",
    "print(f'Targets retrieved from {csv_file_path}, and aligned with pdbs files.\\n\\\n",
    "    There are {len(pdb_targets)} targets values and {len(clusters)} cluster values.\\\n",
    "    Total number of clusters is {len(set(clusters))}.\\n')\n",
    "\n",
    "queries - QueryCollection()\n",
    "for it, pdb in enumerate(pdbs_list):\n",
    "\tqueries.add(\n",
    "\t\tProteinProteinInterfaceResidueQuery(\n",
    "\t\t\tpdb_path = pdb, \n",
    "\t\t\tchain_id1 = \"M\",\n",
    "\t\t\tchain_id2 = \"P\",\n",
    "\t\t\tdistance_cutoff = distance_cutoff,\n",
    "\t\t\ttargets = {\n",
    "\t\t\t\t'binary': pdb_targets[it][0], # binary target value\n",
    "\t\t\t\t'BA': pdb_targets[it][1], # continuous target value\n",
    "\t\t\t\t'cluster': clusters[it]\n",
    "\t\t\t\t},\n",
    "\t\t\tpssm_paths = {\n",
    "\t\t\t\t\"M\": pssm_m[it],\n",
    "\t\t\t\t\"P\": pssm_p[it]\n",
    "\t\t\t\t})\n",
    "\t)\n",
    "print(f'Queries created and ready to be processed.\\n')\n",
    "\n",
    "output_paths = queries.process(process_count = 1)\n",
    "print(output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing hdf5 file in train, valid, test\n",
    "hdf5_path = 'residue.hdf5'\n",
    "train_clusters = [3, 4, 5, 2]\n",
    "val_clusters = [1, 8]\n",
    "test_clusters = [6]\n",
    "target = 'target_values'\n",
    "feature = 'cluster'\n",
    "\n",
    "clusters = {}\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "# '/Users/giuliacrocioni/remote_snellius/data/pMHCI/features_output_folder/GNN/residue/13072022/residue.hdf5'\n",
    "with h5py.File(hdf5_path, 'r') as hdf5:\n",
    "\n",
    "    for key in hdf5.keys():\n",
    "        feature_value = float(hdf5[key][target][feature][()])\n",
    "        if feature_value in train_clusters:\n",
    "            train_ids.append(key)\n",
    "        elif feature_value in val_clusters:\n",
    "            val_ids.append(key)\n",
    "        elif feature_value in test_clusters:\n",
    "            test_ids.append(key)\n",
    "\n",
    "        if feature_value in clusters.keys():\n",
    "            clusters[int(feature_value)] += 1\n",
    "        else:\n",
    "            clusters[int(feature_value)] = 1\n",
    "\n",
    "\n",
    "    print(f'Trainset contains {len(train_ids)} data points, {round(100*len(train_ids)/len(hdf5.keys()), 2)}% of the total data.')\n",
    "    print(f'Validation set contains {len(val_ids)} data points, {round(100*len(val_ids)/len(hdf5.keys()), 2)}% of the total data.')\n",
    "    print(f'Test set contains {len(test_ids)} data points, {round(100*len(test_ids)/len(hdf5.keys()), 2)}% of the total data.\\n')\n",
    "\n",
    "    for (key, value) in dict(sorted(clusters.items(), key=lambda x:x[1], reverse=True)).items():\n",
    "        print(f'Group with value {key}: {value} data points, {round(100*value/len(hdf5.keys()), 2)}% of total data.')\n",
    "\n",
    "save_hdf5_keys(hdf5_path, train_ids, 'train.hdf5', hardcopy = True)\n",
    "save_hdf5_keys(hdf5_path, val_ids, 'valid.hdf5', hardcopy = True)\n",
    "save_hdf5_keys(hdf5_path, test_ids, 'test.hdf5', hardcopy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating variants.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_queries = 5\n",
    "pdb_path = str(PATH_TEST / \"data/pdb/3C8P/3C8P.pdb\")\n",
    "ref_path = str(PATH_TEST / \"data/ref/3C8P/3C8P.pdb\")\n",
    "targets = compute_targets(pdb_path, ref_path)\n",
    "queries = QueryCollection()\n",
    "\n",
    "for number in range(1, count_queries + 1):\n",
    "    query = SingleResidueVariantResidueQuery(\n",
    "        pdb_path,\n",
    "        \"A\",\n",
    "        number,\n",
    "        None,\n",
    "        alanine,\n",
    "        phenylalanine,\n",
    "        pssm_paths={\n",
    "            \"A\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.A.pdb.pssm\"),\n",
    "            \"B\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.B.pdb.pssm\")},\n",
    "        targets = targets\n",
    "    )\n",
    "    queries.add(query)\n",
    "\n",
    "output_paths = queries.add(queries, process_count = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating atomic.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "chain_id1 = \"A\"\n",
    "chain_id2 = \"B\"\n",
    "pdb_paths = [\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\")]\n",
    "\n",
    "queries = QueryCollection()\n",
    "\n",
    "for pdb_path in pdb_paths:\n",
    "    # Append data points\n",
    "    targets = compute_targets(pdb_path, ref_path)\n",
    "    queries.add(ProteinProteinInterfaceAtomicQuery(\n",
    "        pdb_path = pdb_path,\n",
    "        chain_id1 = chain_id1,\n",
    "        chain_id2 = chain_id2,\n",
    "        targets = targets,\n",
    "        pssm_paths = {\n",
    "            chain_id1: pssm_path1,\n",
    "            chain_id2: pssm_path2\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Generate graphs and save them in hdf5 files\n",
    "output_paths = queries.process(process_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('deeprankcore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9044f9e9bb2984c6e66c1f9356a97652bc688d95c3fd9413d1ef7214abf67660"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
